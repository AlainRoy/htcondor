\section{\label{sec:DAGMan}Interjob Dependencies: DAGMan Meta-Scheduler}

The Directed Acyclic Graph Manager (DAGMan) is a meta-scheduler for Condor
jobs.  DAGMan is responsible for submitting batch jobs in a predefined order
and processing the results. A configuration file is defined prior to execution
of DAGMan in which the jobs, their \textit{CondorConfigFile}, and job
dependencies are declared.

The importance of such a tool lies in the fact that the user is able to define
the execution order of a number of Condor Jobs. Just as Condor schedules
condor jobs, DAGMan schedules a system of jobs. In essence, it defines a
problem. Solving a problem may require multiple condor jobs that need data
from each other. This is best represented using a Directed Acyclic Graph
(DAG), which represents the flow of control from one node to another (i.e.,
from one condor job to another) through arrows.

From the point of view of the user, the scheduler is initialized with the
order of execution of jobs, and then started. DAGMan is responsible for all
scheduling, recovery and reporting activities of the submitted system of jobs.

The following sections explain the use of DAGMan in full detail.  However, if
the user only wants the bare essentials, please read
section~\ref{dagman:essentials} to get started more quickly.

\subsection{DAG Input File}

For developers, a useful analogy might be to think of the DAGMan input file as
a makefile, and DAGMan itself as the make executable.  However, DAGMan differs
from make.  Instead of looking at file modification timestamps, DAGMan reads
the common Condor log file generated by the Condor jobs in the DAG to find out
which jobs are unsubmitted, submitted, or complete.  DAGMan also makes a
guarantee that a DAG is recoverable, even if the machine running DAGMan goes
down during execution.

\subsubsection{\label{dagman:dagdesc}Description}

Job dependencies are defined prior to execution of the DAGMan program, using a
DAG input file.  An example input configuration file name is
\File{diamond.dag}.  The input file is read completely, and the DAG data
structure is constructed in memory before the first job is submitted.  With
the exception of the \textit{CondorCommandFile} (see below), the input file is
case insensitive.

Throughout the input file, comments can be placed.  Legal comments exist on a
single line which immediately starts with a `\texttt{\#}' character, followed
by any characters up to the end of line.

It is interesting to note that the DAGMan input file does not contain any
specifics about the individual jobs. Each condor job by itself is handled as
if DAGMan was not present (this includes compiling and linking of the
job). The executable and the input/output parameters for each job are
contained in the CondorCommandFile.  The DAG file merely describes the
relationship between the different condor jobs using the semantics just
described.

\begin{description}

\item[Job Section]

The Job Section of the input DAG file declares all the jobs that will appear
in the DAG.  Each job is described by a single line called a Job Entry.  The
following syntax is used:

\begin{verbatim}
	JOB <JobName> <CondorCommandFile>
\end{verbatim}

The \texttt{JOB} keyword (shown here in upper case only for clarity) declares
this line will map a \textit{JobName} to a Condor Command File.  The
\textit{JobName} is used by DAGMan to uniquely identify jobs throughout the
input file and to name them in output messages.  The
\textit{CondorCommandFile} is the input file used by \Condor{submit} to run
the individual condor job.  Because the Unix file system is case sensitive,
the case of the \textit{CondorCommandFile} is preserved.

The JobName can be any string that contains no white space.  The JobName is
not case sensitive, so ``JobA'' is equivalent to ``joba''.  An example
\textit{CondorCommandFile} name is \File{a.condor}.  Some important
restrictions are placed on the contents of the \textit{CondorCommandFile},
which will be discussed later.

The user can also have the option of declaring a job as being already
completed in the DAG input file. This may be useful in situations where the
user wishes to verify results, but does not need the entire job dependency
graph to be executed. This is done by adding the word "DONE" to the end of the
Job declaration line.

\begin{verbatim}
	JOB <JobName> <CondorCommandFile> DONE
\end{verbatim}

The \texttt{DONE} feature is utilized when generating a Rescue DAG, a feature
discussed in section~\ref{dagman:rescue}.

\item[PRE/POST Script Section]

Sometimes it is useful to run a setup script before a job is run.  This is
called a PRE script, because it runs before the job is submitted.  A cleanup
script, called the POST script, runs after the job successfully completes.
This is often done when files must be put into a staging area for the job to
use, and then cleaned up or removed once the job in finished running.  Note:
PRE and POST are referred to here as ``scripts'', but they can be any
legitimate executable, even a compiled binary.

A plausible PRE/POST example would be for the PRE script to read compressed
input files from a tape drive, and uncompress them to a staging area (probably
the job's current directory).  The job would read these input files and
produce output files.  The POST script would compress the output files to
tape and then delete the staged input and output files.

Do not think of the PRE and POST scripts as Condor jobs, as if they were nodes
in the DAG of equal standing to other Condor jobs.  They are not so
prestigious.  PRE and POST scripts are run quietly as part of a job's task.
Think of the job and its PRE/POST scripts as a solid indivisible node.  So if
job B depends on job A, then A and both its scripts must complete before any
work is done on B.  It is not correct for B's PRE script to run before or at
the same time as A's POST script, because it is very possible that B's PRE
script expects files that are produced by A's POST!

The script section of the DAG input file allows the option to run a PRE and
POST script for each job in the DAG.  The scripts are optional for each job,
and they both are run on the schedd to which the DAGMan was submitted.  The
job, of course, is being run on a remote machine.

The job will be run regardless of whether a PRE and/or POST script is
specified for it.  However, DAGMan takes note of the exit value of both
scripts, as well as the job.  If any of the three fail (exit with a non-zero
value) DAGMan will act as follows:

\begin{enumerate}

\item If the PRE script fails (exit value != 0), then neither the job nor
the POST script runs, and the job node is marked as failed.  Otherwise, if the
PRE script succeeds, the job is submitted.

\item If the job fails (exit value != 0), the job node is marked as failed.
The POST script is run regardless of the job's return value.  However, it is
possible to pass the job's return value to the post script, so that the post
script can act intelligently.  See the syntax below.

\item If the POST script fails (exit value != 0), then the job is marked as
failed.  Otherwise, the job node is successfully completed.

\end{enumerate}

Suppose the machine running DAGMan crashes.  On recovery, it is possible that
failure happened during 1, 2, or 3.  DAGMan does not log the completion of
scripts, so they are unconditionally run on recovery.  Therefore you should
program your PRE/POST scripts to be tolerant of target files already existing,
in case the script was fully or partially run before.

A single line in the input file specifies either a PRE or POST script.  If
both scripts are desired, then two lines are used.

\begin{verbatim}
	SCRIPT PRE  <JobName>  pre.csh
	SCRIPT POST <JobName> post.csh
\end{verbatim}

The \texttt{SCRIPT} keyword declares the script.  The \texttt{PRE} and
\texttt{POST} keywords specify which script is being described.  The
\textit{JobName} specifies the job to which this script is attached.  The rest
of the line, specifies the script to be executed, optionally followed by any
command line arguments for that script.  The script name and arguments are
case-preserved, for obvious reasons.

It is possible you might want to write a common PRE or POST script for several
jobs.  DAGMan supplies two variables that help you write scripts which can be
told information about the job for which they are running.  The variables are
placed anywhere in the arguments after the script name.

The \texttt{\$JOB} variable evaluates to \textit{JobName}, the name of the job
owning this script.  Suppose your pre script is to decompress a file called
\textit{JobName}.gz.  The DAG entry for jobs A, B, and C would be:

\begin{verbatim}
	SCRIPT PRE  A  pre.csh $JOB .gz
	SCRIPT PRE  B  pre.csh $JOB .gz
	SCRIPT PRE  C  pre.csh $JOB .gz
\end{verbatim}

And the \texttt{pre.csh} script might look like the following:

\begin{verbatim}
	#!/bin/csh
	gunzip $argv[1]$argv[2]
\end{verbatim}

Note: When time is available, we will incorporate a better regular
expression parser that can handle a string like \texttt{\$\{JOB\}.gz}, thus
eliminating the need for two arguments.

The second variable only useful for the POST script is \texttt{\$RETURN}, which
evaluates to the return value of the job.  Remember that the POST script is
run unconditionally, regardless of how the job exits.  So the \texttt{\$RETURN}
variable allows the POST script to react intelligently to a failed job.  Note:
If the job is killed, the value of \texttt{\$RETURN} will be -1.

\item[Dependency Section]

The dependency section of the DAG input file follows the Job Section and
describes the dependencies between the jobs listed in the Job Section.  The
notion of a ``parent'' and ``child'' job is introduced here.  A parent job
produces output which is required by one or more child jobs.  None of the
children can run until the parent successfully terminates.  A child job is one
whose input is taken from one or more parent jobs.  The child job cannot run
until all of its parents have successfully terminated.

A single line in the input file can specify the dependencies from one or more
parents to one or more children.

\begin{verbatim}
	PARENT <ParentJobName>* CHILD <ChildJobName>*
\end{verbatim}

The \texttt{PARENT} keyword is followed by one or more
\textit{ParentJobName}s.  Those are followed by the \texttt{CHILD} keyword,
which is followed by one or more \textit{ChildJobName}s.  Each child job
depends on each and every parent job on this line.  So the line
``\texttt{PARENT p1 p2 CHILD c1 c2}'' would produce four dependencies.

\end{description}

\subsubsection{Example}

The following \File{diamond.dag} DAG input file shown below is illustrated in
Figure~\ref{fig:dagman-diamond}.

\begin{verbatim}
	# Filename: diamond.dag
	#
	Job  A  A.condor 
	Job  B  B.condor 
	Job  C  C.condor	
	Job  D  D.condor
	Script PRE  A top_pre.csh
	Script PRE  B mid_pre.perl  $JOB
	Script POST B mid_post.perl $JOB $RETURN
	Script PRE  C mid_pre.perl  $JOB
	Script POST C mid_post.perl $JOB $RETURN
	Script PRE  D bot_pre.csh
	PARENT A CHILD B C
	PARENT B C CHILD D
\end{verbatim}

\begin{figure}[hbt]
\centering
\includegraphics{user-man/dagman-diamond.eps}
\caption{\label{fig:dagman-diamond}Diamond DAG}
\end{figure}

First let us discuss the job's as nodes, ignoring the PRE and POST scripts
for a moment.  With \File{diamond.dag}, job A must execute first, because all
other jobs directly or indirectly depend on it.  After job A successfully
completes, both job B and C are eligible to run.  In fact, they will be
submitted at the same time and hopefully Condor will find two remote hosts
that can run them in parallel.  Since job D depends on both B and C, it must
wait for both to complete successfully before it can be submitted.

Now let us address the PRE and POST scripts.  Remember that the existence of
these script in no way changes the order in which the jobs can be submitted,
nor does it change the dependency tree of the job nodes.  The scripts only
perform tasks right before job submission, and right after job termination.

Job A runs it own PRE script \File{top\_pre.csh}.  Job's B and C share PRE and
POST scripts \File{mid\_pre.perl} and \File{mid\_post.perl}.  The Perl script
knows the job for which it is being called, because it expects the Job's
name as its first argument, given by \texttt{\$JOB}.  Notice that the POST
script is to take one integer argument, the exit value of the job.  Finally
job D runs the PRE script \File{bot\_pre.csh} with no argument.  Since jobs
A and D are missing POST scripts, no script runs after they terminate.

\subsection{Execution}

\subsubsection{\label{dagman:prepjob}Preparing Jobs}

Each individual job in a DAG is free to be a unique executable, with a unique
\textit{CondorCommandFile}.  The DAG can contain a mixture of standard and
vanilla jobs, or even other meta-scheduler jobs, like DAGMan.  On the other
hand, the jobs in the DAG could all use the same executable, or even the same
\textit{CondorCommandFile}.  Anything between both extremes is possible.
However, two limits are imposed.

\begin{enumerate}

\item Each \textit{CondorCommandFile} must submit a cluster of size one.
There cannot be multiple \texttt{queue} lines.  The reasoning is long winded,
so a brief summary will be attempted.  If multi-job clusters were allowed,
DAGMan would have to parse the \textit{CondorCommandFile} to find out how many
jobs belong to that cluster.  Otherwise, DAGMan would not know for sure if a
cluster had terminated based on seeing the event from one job of that
cluster.  This restriction may be lifted in future DAGMan version, depending
on the design and implementation issues.

\item All \textit{CondorCommandFile}s of a DAG must specify the same log.
In order for DAGMan to follow the order of events correctly, all events from
all jobs in the DAG must be sent to the same log file.  This restriction will
be loosened in later versions (see section~\ref{dagman:version}).

\end{enumerate}

For this example, we will write a single \textit{CondorCommandFile} to be used
by all three jobs in the DAG.  Thus, each job will run the same executable.
Notice, that to use the same \textit{CondorCommandFile} and still get
unique filenames for our output, we use the \MacroU{cluster} macro.
Since each job is submitted separately, into its own cluster, this
will provide unique names for our output files.
Otherwise, the jobs would be clobbering each other's output.  

\begin{verbatim}
	# Filename: diamond_job.condor
	#
	executable   = /path/diamond.exe
	output       = diamond.out.$(cluster)
	error        = diamond.err.$(cluster)
	log          = diamond_condor.log
	universe     = vanilla
	notification = NEVER
	queue
\end{verbatim}

Note that notification is set to \texttt{NEVER}.  This is recommended if you
prefer not to have Condor send you e-mail for every job in a large DAG.

\subsubsection{\label{dagman:writedag}Writing the DAG File}

The DAG file names the jobs, associates jobs with their
\textit{CondorCommandFile}, and declares job dependencies.  For our Diamond
example, all four jobs will use the same \File{diamond\_job.condor} file
written earlier.

However, a more typical DAG file would have some different 
\textit{CondorCommandFile} entries, since, presumably, some of the
jobs in your DAG differ beyond where their output goes.
If not, you probably don't even need DAGMan, you just need to submit a
cluster for all your jobs.

\begin{verbatim}
	# Filename: diamond.dag
	# Diamond DAG File for DAGMan
	#
	Job  A  diamond_job.condor
	Job  B  diamond_job.condor
	Job  C  diamond_job.condor
	Job  D  diamond_job.condor
	Script PRE  A  pre.sh $job
	Script POST A post.sh $job $return
	Script PRE  B  pre.sh $job
	Script POST B post.sh $job $return
	Script PRE  C  pre.sh $job
	Script POST C post.sh $job $return
	Script PRE  D  pre.sh $job
	Script POST D post.sh $job $return
	PARENT A CHILD B C
	PARENT B C CHILD D
\end{verbatim}

This DAG file will be the input file for the \Condor{dagman} program.

\subsubsection{\label{dagman:submitdag}Submitting the DAG to Condor}

In order to guarantee recoverability, the DAGMan program itself is run as a
Condor job.  However, DAGMan is not submitted as a standard universe or
vanilla universe job.  Instead, it is run as a meta-scheduler.  Standard and
vanilla universe jobs are usually submitted to the local schedd, which
schedules them for execution on some remote machine in the pool that is idle.
A meta-scheduler is also submitted to the local schedd, but runs on the local
schedd.  The meta-scheduler then submits jobs, according to its design, to the
same local schedd, just as if the user submitted them manually.  In fact, the
local schedd does not know the difference between DAGMan submitting a job, and
the user who originally submitted DAGMan, and could have submitted the DAG
jobs manually.

A DAG is submitted using the \Condor{submit\_dag} script.  For
example, to submit the \File{diamond.dag} DAG to Condor, simply type
``{\tt \Condor{submit\_dag} \File{diamond.dag}}''.
This script will generate the \File{diamond.dag.condor.sub}
\textit{CondorCommandFile} for the DAG, and submit it to Condor.
If the user prefers to edit the \File{diamond.dag.condor} file before
it is submitted to Condor (for example, to change the pre-chosen
filenames), she can issue ``{\tt \Condor{submit\_dag} -no\_submit
\File{diamond.dag}}'', which specifies that \File{diamond.dag.condor}
is generated, but not submitted to Condor.
To run the DAG, issue the command \Condor{submit}
diamond.dag.condor.sub. 

Normally, \Condor{submit\_dag} will try to check your DAG input file
for correctness.
In particular, it tries to verify that all the jobs in your DAG
specify the same log file (which is needed for DAGMan to properly
function, as described above).
If it finds a problem (a job that's using a different log file), it
will print out an error message and abort.
If you use the ``{\tt -verbose}'' option, it will also print out a list of
all jobs in your DAG and the corresponding log file each uses.
However, in some situations, you may not want this check.
For example, if you have a very large DAG (with thousands of jobs),
opening up each submit file to verify its correctness might take quite
a while.
So, if you want to avoid this check, make sure you're using the same
log file for all jobs in your DAG, and then just pass this log file's
name to \Condor{submit\_dag} with the ``{\tt -log <filename>}'' option.
This tells \Condor{submit\_dag} not to bother trying to verify what
log file to use, and it just uses what you tell it.

The ``{\tt -maxjobs <int>}'' option can be used to specify the maximum number
of jobs that DAGMan can submit concurrently.  This would primarily be used to
guarantee that no more than a set number of jobs can be running at once.  A
common example would be a limited amount of input file staging capacity.
Suppose you know each job will require 4 MB of input files, and the jobs are
running in a directory whose volume only has 100 MB of free space.  You might
use the option ``{\tt -maxjobs 25}'' to guarantee that only 25 jobs can be in
the Condor system at one time.  Note that omitting the ``{\tt -maxjobs}''
option is equivalent to specifying ``{\tt -maxjobs 0},'' which means to have
an unlimited number of jobs in Condor at once.

\subsection{Removal}

After submitting a DAG, the user may change her mind and wish to remove the
entire DAG, plus any jobs submitted by that DAG which happen to currently be
running.  DAG removal is easily accomplished by issuing a \Condor{rm} on the
DAGMan job itself.  The schedd sends a special signal to the meta-scheduler,
telling it to remove any of its Condor jobs (using \Condor{rm}) that are
currently running.

However, if the machine is scheduled to go down, and the schedd receives a
shutdown command from the master, the schedd will send a running DAGMan job a
similar shutdown, which instructs DAGMan to clean up memory and exit.
However, in this case, DAGMan does not remove its submitted jobs, but rather
expects them to persistently exist in the Condor queue after restart.

The important thing to remember is that DAGMan will not explicitly run
\Condor{rm} on its jobs except as a result of the user running \Condor{rm} on
the DAGMan job.

\subsection{Recovery}

The Condor system offers the benefit of recoverability, in that if any host
crashes, Condor jobs that were running can be recovered, either by continuing
from the last checkpoint, or rerunning from scratch.  In any event, Condor
guarantees that once a job is successfully submitted, the Condor system will
not loose it.

DAGMan makes the same guarantee about the DAG as a whole.  If the machine
running DAGMan goes down or crashes, upon restart DAGMan will be restarted,
and the state of the DAG jobs will be recovered from the log file
(\File{diamond.dag.condor.log} from our example before).  DAGMan knows to
recover a DAG (as apposed to starting a new one) because it will detect the
existence of a lock file that was not removed from the last run.  If DAGMan
successfully finishes a DAG, the lock file is removed, so that the next run
will not go into recover mode.  The lock file is specified via command-line
argument to DAGMan in the \textit{CondorCommandFile}.  Refer to
section~\ref{dagman:submitdag}.

\subsubsection{\label{dagman:rescue}Rescue DAG}

Currently DAGMan does not support job resubmission.  If any job in the DAG
fails (crashes or returns a non-zero value), the entire DAG must be aborted.
Future versions must implement non-trivial logging mechanisms to make
resubmission possible.  Until that time arrives, DAGMan offers a ``poor
man's'' approach to resubmission, the Rescue DAG.

The Rescue DAG is generated whenever DAGMan exits because of a failed job.  At
exit time, DAGMan generates a secondary DAG file, which is functionally the
same as the original DAG file, except that successfully finished jobs are
marked as {\tt DONE}.  If the Rescue DAG is used as the input DAG for the next
iteration of DAGMan, none of the jobs marked as {\tt DONE} will be
resubmitted.  Therefore, work done so far is not lost.

The Rescue DAG is automatically generated by DAGMan in the event of a job
failure.  If the name of the original DAG is \File{diamond.dag}, then the
rescue DAG will have the name \File{diamond.dag.rescue}.  If you view the
Rescue DAG with your favorite text viewer, you will see comments at the top
giving statistics about the failed run, such as the total number of jobs, the
number marked as {\tt DONE}, the number that failed, and a list of the names
of the jobs that failed.

The assumption made is that you will fix the problem occurring in the failed
jobs, and then rerun the DAG by renaming the rescue DAG to the original DAG.  
For example, suppose we run the diamond DAG example above, and job C fails.
Here is what the iteration might look like:

\begin{verbatim}
	unix% condor_submit_dag diamond.dag
\end{verbatim}
(receive e-mail that DAGMan is complete, check for Rescue DAG)
\begin{verbatim}
	unix% head -12 diamond.dag.rescue

	# Rescue DAG file, created after running
	#   the diamond.dag DAG file
	#
	# Total number of jobs: 4
	# Jobs premarked DONE: 2
	# Jobs that failed: 1
	#   C

	Job  A  diamond_job.condor DONE
	Job  B  diamond_job.condor DONE
	Job  C  diamond_job.condor
	Job  D  diamond_job.condor

	unix% mv diamond.dag.rescue diamond.dag
	unix% condor_submit_dag diamond.dag
\end{verbatim}

After resubmitting the DAG, jobs A and B are skipped, since they are premarked
as {\\tt DONE}.  Only jobs C and D are actually submitted.  Thus redundant work is
avoided.

\subsection{\label{dagman:essentials}Essentials}

This section is written for those users looking for the boiled down,
absolutely essential steps to successfully submit a DAG.

\begin{description}

\item[Prepare Jobs] 
Each \textit{CondorCommandFile} can only submit one job.
Multi-job clusters (multiple \texttt{queue} lines) are not supported.
The \texttt{log=} for all \textit{CondorCommandFile}s must point to
the same Condor log file, otherwise, DAGMan will not see all the
Condor log entries for every job in the DAG.  
Refer to section~\ref{dagman:prepjob} for details on how to prepare
jobs.

\item[Write DAG File] Write the DAG file, so that JOB entries refer to the
\textit{CondorCommandFile}s you wrote in the previous step.  Refer to
section~\ref{dagman:writedag} to learn about writing a DAG file.

\item[Submit the DAG] Finally, you submit the DAG written in the previous step
using the \Condor{submit\_dag} script.  Refer to
section~\ref{dagman:submitdag}.

\end{description}


\subsection{\label{dagman:version}Version Summary}

This section addresses the features and limitations that exist in the current
version of DAGMan, and how they may change in future versions.

This first public release of DAGMan was written and tested in the Condor 6.1.0
environment.  Originally the plan was to release it as a separate contribution
module.  However, DAGMan functionality depends on the implementation of the
schedd and \Condor{submit}, which sometimes changes from release to release of
Condor.  Therefore, DAGMan is also release as part of each Condor release.

A reasonable effort has been made to test large DAGS (on the order of 5000
jobs) on Solaris x86 and Sparc.  Users are encouraged to send e-mail to
\Email{condor-admin@cs.wisc.edu} if a bug is found.

The following feature summary compares the current version with future
versions of DAGMan still to come.  All of these new features will likely be
seen with Condor 6.3 development releases.

\begin{description}
\item[Feature] : Command Socket
\item[Current Version] : Unsupported
\item[Future Versions] : A general purpose command socket will be used to
direct Dagman while it's running.  Commands like CANCEL\_JOB X or DELETE\_ALL
would be supported, as well as notification messages like JOB\_SUBMIT or
JOB\_TERMINATE, etc.  Eventually, a Java Gui would graphically represent the
Dag's state, and offer buttons and dials for graphic Dag manipulation.
\end{description}

\begin{description}
\item[Feature]: DAG removal
\item[Current Version]: Supported via \Condor{rm} of the DAG.
\item[Future Versions]: Supported by a command socket such as DELETE\_ALL
\end{description}

\begin{description}
\item[Feature]: Condor Log File
\item[Current Version]: All jobs in a DAG must specify the same Condor log
file.  That Condor log file must be unique.  No other DAGs or Condor jobs can
point to that log file.
\item[Future Versions]: All jobs in a Dag must go to one log file, but
log file can be shared with other Dags and Condor jobs.
\end{description}

\begin{description}
\item[Feature]: Job UNDO
\item[Current Version]: All jobs must exit normally, else the DAG will be
aborted.
\item[Future Versions]: A job can be ``undone'', or there is some
notion of a job instance.  Hence, a job that exits abnormally or is
cancelled by the user can be rerun such that the new run's log entry
is unique from the old run's log entry (in terms of recovery)
\end{description}
