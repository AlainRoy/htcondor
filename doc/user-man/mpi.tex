%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\label{sec:MPI}Running MPICH jobs in Condor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In addition to PVM, Condor also supports the execution of parallel jobs
that utilize MPI.
Our current implementation supports the following features:
\begin{itemize}
\item There are no alterations to the MPICH implementation.  You can directly
use the version from Argonne National Labs.

\item You do not have to re-compile or re-link your MPICH job.  Just
compile it using the regular \Prog{mpicc}.  Note that you have to be using
the ch\_p4 subsystem provided by Argonne.

\item The communication speed of the MPI nodes is not affected by 
running it under Condor.
\end{itemize}
However, there are some limitations to our current implementation.

\subsection{\label{sec:MPI-caveats}Caveats}
\begin{description}
\item[MPICH] Your MPI job must be compiled with MPICH, Argonne National
Labs' implementation of MPI.  Specifically, you must use the ``ch\_p4'' 
device for MPICH.  For information on MPICH, see Argonne's web page
at \Url{http://www-unix.mcs.anl.gov/mpi/mpich/}. Your version of MPICH must
not be compiled with the path to RSH hard-coded into the library 
(As a result of running configure as \verb ./configure -rsh=/path/to/your/rsh
possilbly.) Condor provides a special version of rsh that it uses to start 
jobs. 

\item[Dedicated Resources] You must make sure that your MPICH jobs
will be running on machines that will not vacate the job before the job
terminates naturally.  (This is a limitation of MPICH and the MPI 
specification.) Unlike PVM (Section~\ref{sec:PVM}), the current MPICH
implementation does not support dynamic resource management.  That is, 
processes in the virtual machine may NOT join or leave the computation at 
any time.  If you start an MPI job with 4 nodes, for example, none of those 4 
nodes can be preempted by other Condor jobs or the machine's owner.
Certain resources in your Condor pool should be configured with this
policy.
There is a whole section in the Administrator's Manual regarding how
to configure resource in a Condor Pool to be dedicated resources which
can execute an MPI application.

\item[Condor Version 6.3.0+] You must be running this version of 
the Condor distribution (or greater) in order to submit MPI jobs.

\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:MPI-submit}Submitting to Condor}

Here is a minimal submit file to submit an MPI job to Condor.  For more 
information on writing submit files, see Section~\ref{sec:sample-submit-files}.

\begin{verbatim}
universe = MPI
executable = your_mpi_program
machine_count = 4
queue 
\end{verbatim}

This tells Condor to start the executable named \File{your\_mpi\_program}
on four machines.  These four machines will be of the same architechture
and operating system as the submitting machine.  Note the 
\verb+universe = MPI+ line tells Condor that an MPICH job is being submitted.  

Now let's try a more sophisticated submit file:
\begin{verbatim}
###################################################################
## submitfile                                                    ##
###################################################################
universe = MPI
executable = simplempi
log = logfile
input = infile.$(NODE)
output = outfile.$(NODE)
error = errfile.$(NODE)
machine_count = 4
queue
\end{verbatim}

Notice the \MacroU{NODE} macro, which is expanded when the job starts so that
it becomes equivalent to the MPI ``id'' of the MPICH job.  The first 
process started becomes ``0'', the second is ``1'', etc.  For example, 
let's say I prepared four input files, named \File{infile.0} through 
\File{infile.3}:
\begin{verbatim}
infile.0: 
Hello number zero.

infile.1: 
Hello number one.
\end{verbatim}
etc.  I then created a simple MPI job, named \File{simplempi.c}
\begin{verbatim}
/******************************************************************
 * simplempi.c
 ******************************************************************/
#include <stdio.h>
#include "mpi.h"

int main(argc,argv)
    int argc;
    char *argv[];
{
    int myid;
    char line[128];

    MPI_Init(&argc,&argv);
    MPI_Comm_rank(MPI_COMM_WORLD,&myid);

    fprintf ( stdout, "Printing to stdout...%d\n", myid );
    fprintf ( stderr, "Printing to stderr...%d\n", myid );
    fgets ( line, 128, stdin );
    fprintf ( stdout, "From stdin: %s", line );

    MPI_Finalize();
    return 0;
}
\end{verbatim}
And to complete the demonstration, here's the \File{Makefile}:
\begin{verbatim}
###################################################################
## This is a very basic Makefile                                 ##
###################################################################

# Change this part to your mpicc, obviously....
CC          = /usr/local/bin/mpicc
CLINKER     = $(CC)

CFLAGS    = -g
EXECS     = simplempi

all: $(EXECS)

simplempi: simplempi.o
        $(CLINKER) -o simplempi simplempi.o -lm

.c.o:
        $(CC) $(CFLAGS) -c $*.c
\end{verbatim}

Once \File{simplempi} is built, use \Condor{submit} to submit your job.
This job should finish pretty quickly once it finds machines to run on,
and the results will be what you expect:  8 files will be created:  
\File{errfile.[0-3]} and \File{outfile.[0-3]}.  For example, \File{outfile.0}
will contain
\begin{verbatim}
Printing to stdout...0
From stdin: Hello number zero.
\end{verbatim}
and \File{errfile.0} will contain
\begin{verbatim}
Printing to stderr...0
\end{verbatim}

Of course, individual tasks may open other files; this example was 
constructed to demonstrate the \MacroUNI{NODE} feature and the setup of
the expected \File{stdin}, \File{stdout}, and \File{stderr} files in the MPI
universe.  
