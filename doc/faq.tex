\index{Condor!FAQ|(}
\index{Condor!Frequently Asked Questions|(}
\index{FAQ|(}
\index{Frequently Asked Questions|(}

This is where you can find quick answers to some commonly asked
questions about Condor.

\section{Obtaining \& Installing Condor}
\index{Condor!downloading|(}
\index{Condor!distribution|(}
\index{Condor!getting|(}
\index{Condor!binaries|(}

\subsection{Where can I download Condor?}

Condor can be downloaded from
\Url{http://www.cs.wisc.edu/condor/downloads} (Madison, Wisconsin,
USA) or \Url{http://www.bo.infn.it/condor-mirror/downloads} (a mirror
site at the Istituto Nazionale di Fisica Nucleare in Bologna, Italy).

\subsection{When I click to download Condor, it sends me back to the downloads page!}

If you are trying to download Condor through a web proxy, try
disabling it.
Our web site uses the ``referring page'' as you navigate through our
download menus in order to give you the right version of Condor, but
sometimes proxies block this information from reaching our web site.

\subsection{What platforms do you support?}

See Section~\ref{sec:Availability}, on
page~\pageref{sec:Availability}.

\index{Condor!binaries|)}

\subsection{Do you distribute source code?}
\index{Condor!source code|(}

At this time we do \Bold{not} distribute source code publicly, but
instead consider requests on a case-by-case basis.
If you need the source code, please email us at
\Email{condor-admin@cs.wisc.edu} explaining why, and we'll get back to
you.

\index{Condor!source code|)}

\subsection{What is ``Personal Condor''?}
\index{Condor!Personal|(}
\index{Personal Condor|(}

Personal Condor is a term used to describe a specific style of Condor
installation suited for individual users who do not have their own
pool of machines, but want to submit Condor jobs to run elsewhere.

A Personal Condor is essentially a one-machine, self-contained Condor
pool which can use ``flocking'' to access resources in other Condor
pools.
See Section~\ref{sec:Flocking}, on page~\pageref{sec:Flocking} for
more information on flocking.

\index{Personal Condor|)}
\index{Condor!Personal|)}
\index{Condor!downloading|)}
\index{Condor!distribution|)}
\index{Condor!getting|)}

\subsection{What do I do now? My installation of Condor does not work.}

What to do to get Condor running properly depends on what sort of
error occurs. 
One common error category are communication errors.
Condor daemon log files report a failure to bind.
Or, the errors in the various log files may be of the form:

\begin{verbatim}
(date and time) Error sending update to collector(s)
(date and time) Can't send end_of_message
(date and time) Error sending UDP update to the collector

(date and time) failed to update central manager

(date and time) Can't send EOM to the collector
\end{verbatim}

This problem can also be observed by running \Condor{status}.
It will give a message of the form:
\begin{verbatim}
Error:  Could not fetch ads --- error communication error
\end{verbatim}

To solve this problem, understand that
Condor uses the first network interface it sees on the machine.
Since machines often have more than one interface,
this problem usually implies that the wrong network
interface is being used.

It is incorrect to use the localhost network interface.
This has IP address 127.0.0.1 on all machines.
To check if this incorrect IP address is being used,
look at the contents of the
CollectorLog file on the pool's
your central manager right after it is started.  
The contents will be of the form:

\begin{verbatim}
5/25 15:39:33 ******************************************************
5/25 15:39:33 ** condor_collector (CONDOR_COLLECTOR) STARTING UP
5/25 15:39:33 ** $CondorVersion: 6.2.0 Mar 16 2001 $
5/25 15:39:33 ** $CondorPlatform: INTEL-LINUX-GLIBC21 $
5/25 15:39:33 ** PID = 18658
5/25 15:39:33 ******************************************************
5/25 15:39:33 DaemonCore: Command Socket at <128.105.101.15:9618>
\end{verbatim}

The last line tells the IP address and port the collector has
bound to and is listening on.
If the IP address is 127.0.0.1, then Condor is definitely using the wrong
network interface.

There are two solutions to this problem.
One solution changes the order of the network interfaces.
The preferred solution
sets which network interface Condor should use
by adding the following parameter to the
local Condor configuration file:

\begin{verbatim}NETWORK_INTERFACE = machine-ip-address\end{verbatim}

Where \verb@machine-ip-address@ is the IP address of the interface you wish
Condor to use.

\section{Setting up Condor}

\subsection{How do I set up a central manager on a machine with multiple
network interfaces?}

Please see section~\ref{sec:Multiple-Interfaces} on 
page~\pageref{sec:Multiple-Interfaces}.

\subsection{How do I get more than one job to run on my SMP machine?}

Condor will automatically recognize a SMP machine and advertise each
CPU of the machine separately.
For more details, see section~\ref{sec:Configuring-SMP} on
page~\pageref{sec:Configuring-SMP}.

\subsection{How do I set up my machines so that only certain users's
jobs will run on them?}

Restrictions on what jobs will run on a given resource can be easily
specified in the resource's Requirements statement.

To specify that a given machine should only run certain users's jobs,
for example, you could add the following Requirements entry to the
machine's Condor configuration file:

\begin{verbatim}Requirements = (RemoteUser == "userfoo@baz.edu" || RemoteUser == "userbar@baz.edu" )\end{verbatim}

To configure multiple machines to do so, simply create a common
configuration file containing this requirement for them to share.

\subsection{How do I configure Condor to run my jobs only on machines
that have the right packages installed?}
\index{running a job!on only certain machines|(}

This is a two-step process.
First, you need to tell the machines to report that they have special
software installed, and second, you need to tell the jobs to require
machines that have that software.

To tell the machines to report the presence of special software, first
add a parameter to their configuration files like so:

\begin{verbatim}HAS_MY_SOFTWARE = True\end{verbatim}

And then, if there are already STARTD\_EXPRS defined in that file, add
HAS\_MY\_SOFTWARE to them, or, if not, add the line:

\begin{verbatim}STARTD_EXPRS = HAS_MY_SOFTWARE, $(STARTD_EXPRS)\end{verbatim}

\Note For these changes to take effect, each \Condor{startd} you update
needs to be reconfigured with \Condor{reconfig} -startd.

Next, to tell your jobs to only run on machines that have this
software, add a requirements statement to their submit files like so:

\begin{verbatim}Requirements = (HAS_MY_SOFTWARE =?= True)\end{verbatim}

\Note Be sure to use =?= instead of == so that if a machine doesn't have
the HAS\_MY\_SOFTWARE parameter defined, the job's Requirements
expression will not evaluate to ``undefined'', preventing it from
running anywhere!

\index{running a job!on only certain machines|)}

\subsection{How do I configure Condor to only run jobs at night?}

A commonly requested policy for running batch jobs is to only allow
them to run at night, or at other pre-specified times of the day.
Condor allows you to configure this policy with the use of the
\Attr{ClockMin} and \Attr{ClockDay} \Condor{startd} attributes.  
A complete example of how to use these attributes for this kind of
policy is discussed in subsection~\ref{sec:Time-Of-Day-Policy} on
page~\pageref{sec:Time-Of-Day-Policy}, ``A Policy for Only Running
Jobs at Certain Times of the Day'', inside
section~\ref{sec:Configuring-Policy} on ``Configuring The Startd
Policy''
\index{running a job!only at night|)}
\index{running a job!at certain times of day|)}


\subsection{Why will the \Condor{master} not run when a local
configuration file is missing?}

If a \Macro{LOCAL\_CONFIG\_FILE} 
is specified in the global configuration file,
but the specified file does not exist,
the \Condor{master} will not start up, and it prints a variation
of the following example message.

\begin{verbatim}
ERROR: Can't read config file /mnt/condor/hosts/bagel/condor_config.local
\end{verbatim}

This is not a bug; it is a feature!
Condor has always worked this way on purpose.
There is a potentially
large security hole if Condor is configured to read from a file that
does not exist.
By creating that file, a malicious user could
change all sorts of Condor settings.
This is an easy way
to gain root access to a machine,
where the daemons are running as root.

The intent is that
if you've set up your global configuration file to read
from a local configuration file, and the local file is not there,
then something is wrong.
It is better for the \Condor{master} to exit right away and
log an error message than to start up.

If the \Condor{master} continued with the local configuration file
missing, either A) someone could breach security or B) you will have
potentially important configuration information missing.
Consider the example where the local configiguration file was on an NFS
partition and the server was down. 
There would be all sorts of
really important stuff in the local configuration file,
and Condor might do bad things if it started without those settings.  

If supplied it with an empty file, the \Condor{master} works fine.

\section{Running Condor Jobs}

\subsection{I'm at the University of Wisconsin-Madison Computer
Science Dept., and I am having problems!}

Please see the web page \Url{http://www.cs.wisc.edu/condor/uwcs}.
As
it explains, your home directory is in AFS, which by default has
access control restrictions which can prevent Condor jobs from running
properly.
The above URL will explain how to solve the problem.

\subsection{I'm getting a lot of email from Condor.  Can I just delete it all?}

Generally you shouldn't ignore \Bold{all} of the mail Condor sends,
but you can reduce the amount you get by telling Condor that you don't
want to be notified every time a job successfully completes, only when
a job experiences an error.
To do this, include a line in your submit file like the following:

\begin{verbatim}Notification = Error\end{verbatim}

See the Notification parameter in the \Condor{q} man page on
page~\pageref{man-condor-submit-notification} of this manual for more
information.

\subsection{Why will my vanilla jobs only run on the machine where I
submitted them from?}

Check the following:
\begin {enumerate}

\item{Did you submit the job from a local filesystem that other
computers can't access?}

See Section~\ref{sec:Shared-Filesystem-Config-File-Entries}, on
page~\pageref{sec:Shared-Filesystem-Config-File-Entries}.

\item{Did you set a special requirements expression for 
vanilla jobs that's preventing them from running but not other jobs?}

See Section~\ref{sec:Shared-Filesystem-Config-File-Entries}, on
page~\pageref{sec:Shared-Filesystem-Config-File-Entries}.

\item{Is Condor running as a non-root user?}

See Section~\ref{sec:Non-Root}, on page~\pageref{sec:Non-Root}.

\end{enumerate}

\subsection{My job starts but exits right away with signal 9.}

\index{job!exiting!signal 9 \(Unix\)|(}

This can occur when the machine your job is running on is missing a
shared library required by your program.
One solution is to install the shared library on all machines the job
may execute on.
Another, easier, solution is to try to re-link your program statically
so it contains all the routines it needs.

\index{job!exiting!signal 9 \(Unix\)|)}

\subsection{Why aren't any or all of my jobs running?}

Problems like the following are often reported to us:

\begin{verbatim}
> I have submitted 100 jobs to my pool, and only 18 appear to be
> running, but there are plenty of machines available.
What should I
> do to investigate the reason why this happens?
\end{verbatim}

Start by following these steps to understand the problem:

\begin{enumerate}

\item Run \Condor{q} -analyze and see what it says.

\item Look at the User Log file (whatever you specified as "log = XXX"
in the submit file).

See if the jobs are starting to run but then exiting right away, or if
they never even start.

\item Look at the SchedLog on the submit machine after it negotiates
for this user.
If a user doesn't have enough priority to get more machines the
SchedLog will contain a message like "lost priority, no more jobs".

\item If jobs are successfully being matched with machines, they
still might be dying when they try to execute due to file permission
problems or the like.
Check the ShadowLog on the submit machine for warnings or errors.

\item Look at the NegotiatorLog during the negotiation for the user.
Look for messages about priority, "no more machines", or similar.

\end{enumerate}

\subsection{Can I submit my standard universe SPARC Solaris 2.6 jobs
and have them run on a SPARC Solaris 2.7 machine?}
\index{Solaris26}
\index{Solaris27}

No. You may only use binary compatibility between SPARC Solaris 2.5.1
and SPARC Solaris 2.6 and between SPARC Solaris 2.7 and SPARC Solaris
2.8, but not between SPARC Solaris 2.6 and SPARC Solaris 2.7.  We may
implement support for this feature in a future release of Condor.

\subsection{Why do my vanilla jobs keep cycling between suspended and
unsuspended?}
\index{vanilla jobs!cycling between suspended and unsuspended}

This is a load sampling error that Condor performs when starting a many
process vanilla job with heavy initial load.
Condor mistakenly decides that the load on the machine has gotten too
high while the job is in the initialization phase and kicks the job off
the machine.

What is needed is a way for Condor to check to see if the load of the machine
has been high over a certain period of time. There is a startd attribute,
\AdAttr{CpuBusyTime} that can be used for this purpose. This macro
returns the time \MacroUNI{CpuBusy}(usually defined in the default
config file) has been true. \MacroUNI{CpuBusy} is defined in terms of
non-Condor load.

To take advantage of this macro, you can use it in your \Macro{SUSPEND}\ macro.
Here is an example:
\begin{verbatim}
SUSPEND = (CpuBusyTime > 3 * $(MINUTE)) && ((CurrentTime - JobStart) > 90)
\end{verbatim}

The above policy says to only suspend the job if the cpu has been busy
with non-Condor load at least three minutes and it has been at least 90
seconds since the start of the job.

\subsection{Why might my job be preempted (evicted)?}

There are four circumstances under which Condor may evict a job.
They are controlled by different expressions.

Reason number 1 is the user priority:
controlled by the \Attr{PREEMPTION\_REQUIREMENTS}
expression in the configuration file.
If there is a job from a 
higher priority user sitting idle,
the \Condor{negotiator} daemon may evict 
a currently running job submitted from a lower priority user if 
\Attr{PREEMPTION\_REQUIREMENTS} is True.
For more on user priorities,
see section~\ref{sec:Priorities} and
section~\ref{sec:UserPrio}.

Reason number 2 is the owner (machine) policy:
controlled by the \Attr{PREEMPT} expression in the configuration file.
When a job is running and the \Attr{PREEMPT} expression
evaluates to True,
the \Condor{startd} will evict the job.
The \Attr{PREEMPT} expression should reflect 
the requirements under which the machine owner will not permit
a job to continue to run.
For example, a policy to evict a currently running job when a key is hit
or when it is the 9:00am work arrival time,
would be expressed in the \Attr{PREEMPT} expression 
and enforced by the \Condor{startd}.
For more on the \Attr{PREEMPT} expression,
see section~\ref{sec:Configuring-Policy}.

Reason number 3 is the owner (machine) preference:
controlled by the \Attr{RANK} expression in the 
configuration file (sometimes called the startd rank or machine rank).
The \Attr{RANK} expression is evaluated as a floating point number.
When one job is running, a second idle job that evaluates to a higher
\Attr{RANK} value 
tells the \Condor{startd} to prefer the second job over the first.
Therefore, the \Condor{startd} will evict the first 
job so that it can start running the second (preferred) job.
For more on \Attr{RANK},
see section~\ref{sec:Configuring-Policy}.

Reason number 4 is if Condor is to be shutdown:
on a machine that is currently running a job.
Condor evicts the currently running job before proceding
with the shutdown.

\section{Condor on Windows NT / Windows 2000}

\subsection{Will Condor work on a network of mixed Unix and NT machines?}

You can have a Condor pool that consists of both Unix and NT machines.

Your central manager can be either Windows NT or Unix.  For example,
even if you had a pool consisting strictly of Unix machines, you could
use an NT box for your central manager, and vice versa.

You can submit jobs destined to run on Windows NT from either an NT
machine \Bold{or} a Unix machine.  However, at this point in time you
cannot submit jobs destined to run on Unix from NT.  We do plan on
adding this functionality, however.

So, in summary:

\begin{enumerate}

\item{A single Condor pool can consist of both Windows NT and Unix
machines.}

\item{It does not matter at all if your Central Manager is Unix or NT.}

\item{Unix machines can submit jobs to run on other Unix or Windows NT
machines.}

\item{Windows NT machines can only submit jobs which will run on Windows
NT machines.}

\end{enumerate}

\subsection{When I run \Condor{status} I get a communication error, or the
Condor daemon log files report a failure to bind.}

Condor uses the first network interface it sees on your machine.
This problem usually means you have an extra, inactive network
interface (such as a RAS dialup interface) defined before to your
regular network interface.

To solve this problem, either change the order of your network
interfaces in the Control Panel, or explicity set which network
interface Condor should use by adding the following parameter to your
Condor config file:

\begin{verbatim}NETWORK_INTERFACE = ip-address\end{verbatim}

Where ``ip-address'' is the IP address of the interface you wish
Condor to use.

\subsection{My job starts but exits right away with status 128.}
\index{job!exiting!status 128 \(NT\)|(}

This can occur when the machine your job is running on is missing a
DLL (Dynamically Linked Library) required by your program.
The solution is to find the DLL file the program needs and put it in
the TRANSFER\_INPUT\_FILES list in the job's submit file.

To find out what DLLs your program depends on, right-click the program
in Explorer, choose Quickview, and look under ``Import List''.

\index{job!exiting!status 128 \(NT\)|)}

\subsection{Why does the startd crash on CondorNT with the error\\
"caInsert: Can't insert CpuBusy into target classad."?}

\index{decimal point!problems with}
This is a common problem with European installations on Windows.
The problem is Condor expects all decimal points to be the
period character (.),
but the Windows locale defines them as the comma character(,).
This will be fixed in the next version of Condor for NT,
however we have users who have fixed the problem by 
changing the following registry value to a period instead of a comma:
\verb@HKEY_USERS\.DEFAULT\Control Panel\International\sDecimal@

\subsection{How can I access network files with Condor on NT?}

Features to allow Condor NT to work well with a network file server are 
coming very soon.  However, there are a couple of work-arounds which you 
can do immediately with the current version of Condor NT in order to access 
a file server.

The heart of the problem is that on the execute machine, Condor creates a 
"temporary" user which will run the job... and your file server has never 
heard of this user before.  So the workaround is to either

\begin{itemize}
\item A: access the file server as a different user via a net use command with a 
login and password
\item B: access the file server as guest
\item C: access the file server with a "NULL" descriptor
\item D: use the contrib module from the folks at Bristol University
\end{itemize}

All of these workarounds have disadvantages, but they may be able to hold 
you until our code to support shared file servers in Condor is officially 
released.

Here are the three methods in more detail:

METHOD A - access the file server as a different user via a net use command 
with a login and password

Example: you want to copy a file off of a server before running it....

\begin{verbatim}
   @echo off
   net use \\myserver\someshare MYPASSWORD /USER:MYLOGIN
   copy \\myserver\someshare\my-program.exe
   my-program.exe
\end{verbatim}

The idea here is to simply authenticate to the file server with a different 
login than the temporary Condor login.  This is easy with the "net use" 
command as shown above.  Of course, the obvious disadvantage is this user's 
password is stored and transferred as cleartext.

METHOD B - access the file server as guest

Example: you want to copy a file off of a server before running it as GUEST

\begin{verbatim}
   @echo off
   net use \\myserver\someshare
   copy \\myserver\someshare\my-program.exe
   my-program.exe
\end{verbatim}

In this example, you'd contact the server MYSERVER as the Condor temporary 
user.  However, if you have the GUEST account enabled on MYSERVER, you will 
be authenticated to the server as user "GUEST".  If your file permissions 
(ACLs) are setup so that either user GUEST (or group EVERYONE) has access 
the share "someshare" and the directories/files that live there, you can 
use this method.  The downside of this method is you need to enable the 
GUEST account on your file server.   \Warn This should be done *with 
extreme caution* and only if your file server is well protected behind a 
firewall that blocks SMB traffic.

METHOD C - access the file server with a "NULL" descriptor

One more option is to use NULL Security Descriptors.  In this way, you
can specify which shares are accessible by NULL Descriptor by adding
them to your registry.  You can then use the batch file wrapper like:

\begin{verbatim}
net use z: \\myserver\someshare /USER:""
z:\my-program.exe
\end{verbatim}

so long as 'someshare' is in the list of allowed NULL session shares.  To
edit this list, run regedit.exe and navigate to the key:

\begin{verbatim}
HKEY_LOCAL_MACHINE\
   SYSTEM\
     CurrentControlSet\
       Services\
         LanmanServer\
           Parameters\
             NullSessionShares
\end{verbatim}

and edit it.  unfortunately it is a binary value, so you'll then need to
type in the hex ascii codes to spell out your share.  each share is
separated by a null (0x00) and the last in the list is terminated with
two nulls.

although a little more difficult to set up, this method of sharing is a
relatively safe way to have one quasi-public share without opening the
whole guest account.  you can control specifically which shares can be 
accessed or not via the registry value mentioned above.


METHOD D -  access with the contrib module from Bristol

Another option: some hardcore Condor users at Bristol University developed 
their own module for starting jobs under Condor NT to access file 
servers.  It involves storing submitting user's passwords on a centralized 
server.  Below I have included the README from this contrib module, which 
will soon appear on our website within a week or two.  If you want it 
before that, let me know, and I could email it to you.

Here is the README from the Bristol Condor NT contrib module:

\begin{verbatim}
README
Compilation Instructions
Build the projects in the following order

CondorCredSvc
CondorAuthSvc
Crun
Carun
AfsEncrypt
RegisterService
DeleteService
Only the first 3 need to be built in order. This just makes sure that the 
RPC stubs are correctly rebuilt if required. The last 2 are only helper 
applications to install/remove the services. All projects are Visual Studio 
6 projects. The nmakefiles have been exported for each. Only the project 
for Carun should need to be modified to change the location of the AFS 
libraries if needed.

Details
CondorCredSvc
CondorCredSvc is a simple RPC service that serves the domain account 
credentials. It reads the account name and password from the registry of 
the machine it's running on. At the moment these details are stored in 
clear text under the key

HKEY_LOCAL_MACHINE\Software\Condor\CredService

The account name and password are held in REG_SZ values "Account" and 
"Password" respectively. In addition there is an optional REG_SZ value 
"Port" which holds the clear text port number (e.g. "1234"). If this value 
is not present the service defaults to using port 3654.

At the moment there is no attempt to encrypt the username/password when it 
is sent over the wire - but this should be reasonably straightforward to 
change. This service can sit on any machine so keeping the registry entries 
secure ought to be fine. Certainly the ACL on the key could be set to only 
allow administrators and SYSTEM access.

CondorAuthSvc and Crun
These two programs do the hard work of getting the job authenticated and 
running in the right place. CondorAuthSvc actually handles the process 
creation while Crun deals with getting the winstation/desktop/working 
directory and grabbing the console output from the job so that Condor's 
output handling mechanisms still work as advertised. Probably the easiest 
way to see how the two interact is to run through the job creation process:

The first thing to realize is that condor itself only runs Crun.exe. Crun 
treats its command line parameters as the program to really run. e.g. "Crun 
\\mymachine\myshare\myjob.exe" actually causes 
\\mymachine\myshare\myjob.exe to be executed in the context of the domain 
account served by CondorCredSvc. This is how it works:

When Crun starts up it gets its window station and desktop - these are the 
ones created by condor. It also gets its current directory - again already 
created by condor. It then makes sure that SYSTEM has permission to modify 
the DACL on the window station, desktop and directory. Next it creates a 
shared memory section and copies its environment variable block into it. 
Then, so that it can get hold of STDOUT and STDERR from the job it makes 
two named pipes on the machine it's running on and attaches a thread to 
each which just prints out anything that comes in on the pipe to the 
appropriate stream. These pipes currently have a NULL DACL, but only one 
instance of each is allowed so there shouldn't be any issues involving 
malicious people putting garbage into them. The shared memory section and 
both named pipes are tagged with the ID of Crun's process in case we're on 
a multi-processor machine that might be running more than one job. Crun 
then makes an RPC call to CondorAuthSvc to actually start the job, passing 
the names of the window station, desktop, executable to run, current 
directory, pipes and shared memory section (it only attempts to call 
CondorAuthSvc on the same machine as it is running on). If the jobs starts 
successfully it gets the process ID back from the RPC call and then just 
waits for the new process to finish before closing the pipes and exiting. 
Technically, it does this by synchronizing on a handle to the process and 
waiting for it to exit. CondorAuthSvc sets the ACL on the process to allow 
EVERYONE  to synchronize on it.

[ Technical note: Crun adds "C:\WINNT\SYSTEM32\CMD.EXE /C" to the start of 
the command line. This is because the process is created with the network 
context of the caller i.e. LOCALSYSTEM. Prepending cmd.exe gets round any 
unexpected "Access Denied" errors. ]

If Crun gets a WM_CLOSE (CTRL_CLOSE_EVENT) while the job is running it 
attempts to stop the job, again with an RPC call to CondorAuthSvc passing 
the job's process ID.

CondorAuthSvc runs as a service under the LOCALSYSTEM account and does the 
work of starting the job. By default it listens on port 3655, but this can 
be changed by setting the optional REG_SZ value "Port" under the registry key

HKEY_LOCAL_MACHINE\Software\Condor\AuthService

(Crun also checks this registry key when attempting to contact 
CondorAuthSvc.) When it gets the RPC to start a job CondorAuthSvc first 
connects to the pipes for STDOUT and STDERR to prevent anyone else sending 
data to them. It also opens the shared memory section with the environment 
stored by Crun.  It then makes an RPC call to CondorCredSvc (to get the 
name and password of the domain account) which is most likely running on 
another system. The location information is stored in the registry under 
the key

HKEY_LOCAL_MACHINE\Software\Condor\CredService

The name of the machine running CondorCredSvc must be held in the REG_SZ 
value "Host". This should be the fully qualified domain name of the 
machine. You can also specify the optional "Port" REG_SZ value in case you 
are running CondorCredSvc on a different port.

Once the domain account credentials have been received the account is 
logged on through a call to LogonUser. The DACLs on the window station, 
desktop and current directory are then modified to allow the domain account 
access to them and the job is started in that window station and desktop 
with a call to CreateProcessAsUser. The starting directory is set to the 
same as sent by Crun, STDOUT and STDERR handles are set to the named pipes 
and the environment sent by Crun is used. CondorAuthSvc also starts a 
thread which waits on the new process handle until it terminates to close 
the named pipes. If the process starts correctly the process ID is returned 
to Crun.

If Crun requests that the job be stopped (again via RPC), CondorAuthSvc 
loops over all windows on the window station and desktop specified until it 
finds the one associated with the required process ID. It then sends that 
window a WM_CLOSE message, so any termination handling built in to the job 
should work correctly.

[Security Note: CondorAuthSvc currently makes no attempt to verify the 
origin of the call starting the job. This is, in principal, a bad thing 
since if the format of the RPC call is known it could let anyone start a 
job on the machine in the context of the domain user. If sensible security 
practices have been followed and the ACLs on sensitive system directories 
(such as C:\WINNT) do not allow write access to anyone other than trusted 
users the problem should not be too serious.]

Carun and AFSEncrypt
Carun and AFSEncrypt are a couple of utilities to allow jobs to access AFS 
without any special recompliation. AFSEncrypt encrypts an AFS 
username/password into a file (called .afs.xxx) using a simple XOR 
algorithm. It's not a particularly secure way to do it, but it's simple and 
self-inverse. Carun reads this file and gets an AFS token before running 
whatever job is on its command line as a child process. It waits on the 
process handle and a 24 hour timer. If the timer expires first it briefly 
suspends the primary thread of the child process and attempts to get a new 
AFS token before restarting the job, the idea being that the job should 
have uninterrupted access to AFS if it runs for more than 25 hours (the 
default token lifetime). As a security measure, the AFS credentials are 
cached by Carun in memory and the .afs.xxx file deleted as soon as the 
username/password have been read for the first time.

Carun needs the machine to be running either the IBM AFS client or the 
OpenAFS client to work. It also needs the client libraries if you want to 
rebuild it.

For example, if you wanted to get a list of your AFS tokens under Condor 
you would run the following:

Crun \\mymachine\myshare\Carun tokens.exe

Running a job
To run a job using this mechanism specify the following in your job 
submission (assuming Crun is in C:\CondorAuth):

Executable= c:\CondorAuth\Crun.exe
Arguments = \\mymachine\myshare\carun.exe 
\\anothermachine\anothershare\myjob.exe
Transfer_Input_Files = .afs.xxx

along with your usual settings.

Installation
A basic installation script for use with the Inno Setup installation 
package compiler can be found in the Install folder.
\end{verbatim}

\subsection{Does Condor run under Windows 2000?}

Condor does run under Windows 2000 Professional and Server.

There will be problems if 
you have more than 2 Gigabytes of RAM or swap space.

A Personal Condor installation will \emph{not} work.

\section{Troubleshooting}

\subsection{What happens if the central manager crashes?} 
\index{crashes|(}
\index{recovery from crashes|(}

If the central manager crashes, jobs that are already running will
continue to run unaffected.
Queued jobs will remain in the queue unharmed, but can not begin
running until the central manager is restarted and begins matchmaking
again.
Nothing special needs to be done after the central manager is brought
back online.

\subsection{When I ssh/telnet to a machine to check particulars of how
Condor is doing something, it is always vacating or unclaimed when I
know a job had been running there!}

Depending on how your policy is set up, Condor will track \emph{any} tty
on the machine for the purpose of determining if a job is to be vacated
or suspended on the machine. It could be the case that after you ssh
there, Condor notices activity on the tty allocated to your connection
and then vacates the job.

\index{crashes|)}
\index{recovery from crashes|)}

\section{Other questions}

\subsection{Is Condor Y2K-compliant?}
\index{Condor!Y2K|(}
\index{Y2K|(}

Yes.
Internally, Condor uses the standard UNIX time representation (the
number of seconds since 1/1/1970) and is not affected by the Y2K bug.
In addition, the Condor tools now correctly display the four-digit
year in their output.

The output of Condor tools from some older versions (pre-6.2) may
display years incorrectly, but their internal representation is still
correct and their display bugs do not affect the operation of Condor.

\index{Condor!Y2K|)}
\index{Y2K|)}

\subsection{Is there a Condor mailing-list?}
\index{Condor!mailing-list|(}
\index{Condor!new versions, notification of|(}

Yes.
We run an extremely low traffic mailing list solely to announce new
versions of Condor.
To subscribe, email \Email{majordomo@cs.wisc.edu} with a message body
of:

\begin{verbatim}subscribe condor-world\end{verbatim}

\index{Condor!mailing list|)}
\index{Condor!new versions, notification of|)}

\subsection{Do you support Globus?}
\index{Globus|(}

Yes, we support a variety of interactions with Globus software,
including running Condor jobs on Globus-managed resources.
At this time, however, we have not released this software publicly.
If you are interested in using Condor with Globus, please send email
to \Email{condor-admin@cs.wisc.edu} and we can provide you with more
information.

\index{Globus|)}

\subsection{What is the process for upgrading from 6.0 to 6.2?}
\index{upgrading!6.0 to 6.2}

The usual process for upgrading is to copy the original 6.0 binaries
to $<$name$>$.old versions and then copy the 6.2 binaries over the 6.0
binaries. The \Condor{master} will automatically(in about 5 minutes)
notice that new binaries have been installed and exec them. Please be sure
to upgrade the config files to match the 6.2 version and pay attention
to copying over your site's specific policy to the new config file. Some
macros and their meanings have changed and need to be addressed in the
upgrade, you may find them in section~\ref{sec:V60-Policy-diffs} of the
manual.

One special note: You must relink your 6.0 standard universe jobs with
the 6.2 supplied libraries. Backwards compatibility to 6.0 from 6.2 had
been intentionally broken because of an evolution of features that the
standard universe now supports.

\subsection{My question isn't in the FAQ!}

If you have any questions that are not listed in this FAQ, try looking
through the rest of the manual.
If you still can't find an answer, feel free to contact us at
\Email{condor-admin@cs.wisc.edu}.

Note that Condor's free email support is provided on a best-effort
basis, and at times we may not be able to provide a timely response.
If guaranteed support is important to you, please inquire about our
paid support services.

\subsection{Why isn't there a section 6.6.6 in the manual?}

There is one now... thanks for asking!


\index{Condor!FAQ|)}
\index{Condor!Frequently Asked Questions|)}
\index{FAQ|)}
\index{Frequently Asked Questions|)}


